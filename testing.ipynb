{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-10T21:47:05.035999100Z",
     "start_time": "2024-11-10T21:47:05.030076900Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key loading successful.\n",
      "Key loading successful.\n"
     ]
    }
   ],
   "source": [
    "from api_keys import fetch_api_key\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = fetch_api_key(\"langsmith\",requires_pass=False)\n",
    "os.environ[\"MISTRAL_API_KEY\"] = fetch_api_key(\"mistral\",requires_pass=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-10T21:47:05.233937100Z",
     "start_time": "2024-11-10T21:47:05.220694200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-10T21:47:05.417672800Z",
     "start_time": "2024-11-10T21:47:05.412154600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "# llm = OllamaLLM(\n",
    "#     model = 'llama2',\n",
    "#     base_url = \"http://localhost:11434\"\n",
    "# )\n",
    "llm = ChatMistralAI(model = \"mistral-large-2402\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-10T21:47:06.111666100Z",
     "start_time": "2024-11-10T21:47:05.584913400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "import feedparser, requests"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-10T21:47:06.115097800Z",
     "start_time": "2024-11-10T21:47:06.114092200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "ARXIV_API_URL = \"http://export.arxiv.org/api/query?\"\n",
    "class ArXivRetriever:\n",
    "    def __init__(self, api_url=\"http://export.arxiv.org/api/query?\"):\n",
    "        self.api_url = api_url\n",
    "\n",
    "    def fetch_papers(self, query, max_results=5):\n",
    "        query_url = f\"{self.api_url}search_query=all:{query}&max_results={max_results}\"\n",
    "\n",
    "        response = requests.get(query_url)\n",
    "        feed = feedparser.parse(response.content)\n",
    "\n",
    "        # Construct a list of dictionaries for JSON-like output\n",
    "        papers = []\n",
    "        for entry in feed.entries:\n",
    "            temp = entry.id.split(\"/\")\n",
    "            pdf_url = \"https://arxiv.org/pdf/\" + temp[-2] + \"/\" + temp[-1] + \".pdf\"\n",
    "            paper = {\n",
    "                \"id\": entry.id.split(\"/\")[-1],\n",
    "                \"title\": entry.title,\n",
    "                \"category\": entry.category,\n",
    "                \"authors\": [author.name for author in entry.authors],\n",
    "                \"author's comment\": entry.get(\"arxiv_comment\", \"No comments available\"),\n",
    "                \"published\": entry.published,\n",
    "                \"summary\": entry.summary,\n",
    "                \"link\": entry.link,\n",
    "                \"pdf_url\": pdf_url,\n",
    "                'journal_ref' : entry.get(\"arxiv_journal_ref\")\n",
    "            }\n",
    "            papers.append(paper)\n",
    "        return papers"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-10T21:47:06.137410Z",
     "start_time": "2024-11-10T21:47:06.115097800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "ret = ArXivRetriever()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-10T21:47:06.150883700Z",
     "start_time": "2024-11-10T21:47:06.139360400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "search_tool = Tool(\n",
    "    name= \"Arxiv_Retriever\",\n",
    "    func= ret.fetch_papers,\n",
    "    description = \"Use this tool to search for research papers related to a topic.\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-10T21:47:06.335184700Z",
     "start_time": "2024-11-10T21:47:06.328180800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "# llm.invoke(\"Hello\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-10T21:47:06.822750200Z",
     "start_time": "2024-11-10T21:47:06.818252400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "responses = search_tool.invoke(\"Attention is all yo need\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-10T21:47:08.324344200Z",
     "start_time": "2024-11-10T21:47:07.309312700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'id': '1506.04932v1',\n  'title': 'General Mixed Multi-Soliton Solutions to One-Dimensional Multicomponent\\n  Yajima-Oikawa System',\n  'category': 'nlin.SI',\n  'authors': ['Junchao Chen', 'Yong Chen', 'Bao-Feng Feng', 'Ken-ichi Maruno'],\n  \"author's comment\": '24 pages,7figs in Journal of the Physical Society of Japan (2015)',\n  'published': '2015-06-16T11:59:22Z',\n  'summary': 'In this paper, we derive a general mixed (bright-dark) multi-soliton solution\\nto a one-dimensional multicomponent Yajima-Oikawa (YO) system, i.e., the\\n(M+1)-component YO system comprised of M-component short waves (SWs) and\\none-component long wave (LW) for all possible combinations of nonlinearity\\ncoefficients including positive, negative and mixed types. With the help of the\\nKP-hierarchy reduction method, we firstly construct two types of general mixed\\nN-soliton solution (two-bright-one-dark soliton and one-bright-two-dark one for\\nSW components) to the (3+1)-component YO system in detail. Then by extending\\nthe corresponding analysis to the (M+1)-component YO system, a general mixed\\nN-soliton solution in Gram determinant form is obtained. The expression of the\\nmixed soliton solution also contains the general all bright and all dark\\nN-soliton solution as special cases. Besides, the dynamical analysis shows that\\nthe inelastic collision can only take place among SW components when at least\\ntwo SW components have bright solitons in mixed type soliton solution. Whereas,\\nthe dark solitons in SW components and the bright soliton in LW component\\nalways undergo usual elastic collision.',\n  'link': 'http://arxiv.org/abs/1506.04932v1',\n  'pdf_url': 'https://arxiv.org/pdf/abs/1506.04932v1.pdf',\n  'journal_ref': None},\n {'id': '1702.04657v1',\n  'title': 'Computational Model for Predicting Visual Fixations from Childhood to\\n  Adulthood',\n  'category': 'cs.CV',\n  'authors': ['Olivier Le Meur',\n   'Antoine Coutrot',\n   'Zhi Liu',\n   'Adrien Le Roch',\n   'Andrea Helo',\n   'Pia Rama'],\n  \"author's comment\": 'No comments available',\n  'published': '2017-02-15T15:48:45Z',\n  'summary': \"How people look at visual information reveals fundamental information about\\nthemselves, their interests and their state of mind. While previous visual\\nattention models output static 2-dimensional saliency maps, saccadic models aim\\nto predict not only where observers look at but also how they move their eyes\\nto explore the scene. Here we demonstrate that saccadic models are a flexible\\nframework that can be tailored to emulate observer's viewing tendencies. More\\nspecifically, we use the eye data from 101 observers split in 5 age groups\\n(adults, 8-10 y.o., 6-8 y.o., 4-6 y.o. and 2 y.o.) to train our saccadic model\\nfor different stages of the development of the human visual system. We show\\nthat the joint distribution of saccade amplitude and orientation is a visual\\nsignature specific to each age group, and can be used to generate age-dependent\\nscanpaths. Our age-dependent saccadic model not only outputs human-like,\\nage-specific visual scanpath, but also significantly outperforms other\\nstate-of-the-art saliency models. In this paper, we demonstrate that the\\ncomputational modelling of visual attention, through the use of saccadic model,\\ncan be efficiently adapted to emulate the gaze behavior of a specific group of\\nobservers.\",\n  'link': 'http://arxiv.org/abs/1702.04657v1',\n  'pdf_url': 'https://arxiv.org/pdf/abs/1702.04657v1.pdf',\n  'journal_ref': None},\n {'id': '1402.0535v1',\n  'title': 'Contribution of Berry Curvature to Thermoelectric Effects',\n  'category': 'cond-mat.mes-hall',\n  'authors': ['Yo Pierre Mizuta', 'Fumiyuki Ishii'],\n  \"author's comment\": 'Comments: 6 pages, SCES2013, accepted for publication in JPS Conf.\\n  Proc',\n  'published': '2014-02-03T22:29:32Z',\n  'summary': 'Within the semiclassical Boltzmann transport theory, the formula for Seebeck\\ncoefficient $S$ is derived for an isotropic two-dimensional electron gas (2DEG)\\nsystem that exhibits anomalous Hall effect (AHE) and anomalous Nernst effect\\n(ANE) originating from Berry curvature on their bands. Deviation of $S$ from\\nthe value $S_0$ estimated neglecting Berry curvarture is computed for a special\\ncase of 2DEG with Zeeman and Rashba terms. The result shows that, under certain\\nconditions the contribution of Berry curvature to Seebeck effect could be\\nnon-negligible. Further study is needed to clarify the effect of additional\\ncontributions from mechanisms of AHE and ANE other than pure Berry curvature.',\n  'link': 'http://arxiv.org/abs/1402.0535v1',\n  'pdf_url': 'https://arxiv.org/pdf/abs/1402.0535v1.pdf',\n  'journal_ref': None},\n {'id': '2306.00454v1',\n  'title': 'Impact of big bang nucleosynthesis on the H0 tension',\n  'category': 'astro-ph.CO',\n  'authors': ['Tomo Takahashi', 'Yo Toda'],\n  \"author's comment\": '14pages, 3figures',\n  'published': '2023-06-01T08:53:26Z',\n  'summary': 'We investigate the impact of big bang nucleosynthesis (BBN) on the Hubble\\ntension, focusing on how the treatment of the reaction rate and observational\\ndata affect the evaluation of the tension. We show that the significance of the\\ntension can vary by $0.8 \\\\sigma$ in some early dark energy model, depending on\\nthe treatment of the reaction rate and observational data. This indicates that\\nhow we include the BBN data in the analysis can give a significant impact on\\nthe Hubble tension, and we need to carefully consider the assumptions of the\\nanalysis to evaluate the significance of the tension when the BBN data is used.',\n  'link': 'http://arxiv.org/abs/2306.00454v1',\n  'pdf_url': 'https://arxiv.org/pdf/abs/2306.00454v1.pdf',\n  'journal_ref': None},\n {'id': '1911.12071v1',\n  'title': 'Jejueo Datasets for Machine Translation and Speech Synthesis',\n  'category': 'cs.CL',\n  'authors': ['Kyubyong Park', 'Yo Joong Choe', 'Jiyeon Ham'],\n  \"author's comment\": 'No comments available',\n  'published': '2019-11-27T10:43:20Z',\n  'summary': 'Jejueo was classified as critically endangered by UNESCO in 2010. Although\\ndiverse efforts to revitalize it have been made, there have been few\\ncomputational approaches. Motivated by this, we construct two new Jejueo\\ndatasets: Jejueo Interview Transcripts (JIT) and Jejueo Single Speaker Speech\\n(JSS). The JIT dataset is a parallel corpus containing 170k+ Jejueo-Korean\\nsentences, and the JSS dataset consists of 10k high-quality audio files\\nrecorded by a native Jejueo speaker and a transcript file. Subsequently, we\\nbuild neural systems of machine translation and speech synthesis using them.\\nAll resources are publicly available via our GitHub repository. We hope that\\nthese datasets will attract interest of both language and machine learning\\ncommunities.',\n  'link': 'http://arxiv.org/abs/1911.12071v1',\n  'pdf_url': 'https://arxiv.org/pdf/abs/1911.12071v1.pdf',\n  'journal_ref': None}]"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-10T21:47:08.327917Z",
     "start_time": "2024-11-10T21:47:08.321889400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "tools = [search_tool]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-10T21:47:08.335290400Z",
     "start_time": "2024-11-10T21:47:08.327917Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-10T21:47:08.578357200Z",
     "start_time": "2024-11-10T21:47:08.573953200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "agent_executor = create_react_agent(llm, tools, checkpointer=memory)\n",
    "config = {\"configurable\": {\"thread_id\":\"abcd\"}}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-10T21:47:08.796978300Z",
     "start_time": "2024-11-10T21:47:08.787375800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-10T21:47:08.974799300Z",
     "start_time": "2024-11-10T21:47:08.968242100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "response = agent_executor.invoke({\"messages\" : [HumanMessage(\"Tell me about vision transformers, using any relevant research paper\")]}, config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-10T21:47:21.833822Z",
     "start_time": "2024-11-10T21:47:09.159444700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "[HumanMessage(content='Tell me about vision transformers, using any relevant research paper', additional_kwargs={}, response_metadata={}, id='e4753bc6-b2b1-4e7e-bf0a-b7762e39dd42'),\n AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'hVebn5ZGU', 'type': 'function', 'function': {'name': 'Arxiv_Retriever', 'arguments': '{\"__arg1\": \"vision transformers\"}'}}]}, response_metadata={'token_usage': {'prompt_tokens': 97, 'total_tokens': 126, 'completion_tokens': 29}, 'model': 'mistral-large-2402', 'finish_reason': 'tool_calls'}, id='run-786a47e7-ebf0-47ea-8ee0-ffcadd7f651d-0', tool_calls=[{'name': 'Arxiv_Retriever', 'args': {'__arg1': 'vision transformers'}, 'id': 'hVebn5ZGU', 'type': 'tool_call'}], usage_metadata={'input_tokens': 97, 'output_tokens': 29, 'total_tokens': 126}),\n ToolMessage(content='[{\"id\": \"2204.07780v1\", \"title\": \"Towards Lightweight Transformer via Group-wise Transformation for\\\\n  Vision-and-Language Tasks\", \"category\": \"cs.CV\", \"authors\": [\"Gen Luo\", \"Yiyi Zhou\", \"Xiaoshuai Sun\", \"Yan Wang\", \"Liujuan Cao\", \"Yongjian Wu\", \"Feiyue Huang\", \"Rongrong Ji\"], \"author\\'s comment\": \"No comments available\", \"published\": \"2022-04-16T11:30:26Z\", \"summary\": \"Despite the exciting performance, Transformer is criticized for its excessive\\\\nparameters and computation cost. However, compressing Transformer remains as an\\\\nopen problem due to its internal complexity of the layer designs, i.e.,\\\\nMulti-Head Attention (MHA) and Feed-Forward Network (FFN). To address this\\\\nissue, we introduce Group-wise Transformation towards a universal yet\\\\nlightweight Transformer for vision-and-language tasks, termed as\\\\nLW-Transformer. LW-Transformer applies Group-wise Transformation to reduce both\\\\nthe parameters and computations of Transformer, while also preserving its two\\\\nmain properties, i.e., the efficient attention modeling on diverse subspaces of\\\\nMHA, and the expanding-scaling feature transformation of FFN. We apply\\\\nLW-Transformer to a set of Transformer-based networks, and quantitatively\\\\nmeasure them on three vision-and-language tasks and six benchmark datasets.\\\\nExperimental results show that while saving a large number of parameters and\\\\ncomputations, LW-Transformer achieves very competitive performance against the\\\\noriginal Transformer networks for vision-and-language tasks. To examine the\\\\ngeneralization ability, we also apply our optimization strategy to a recently\\\\nproposed image Transformer called Swin-Transformer for image classification,\\\\nwhere the effectiveness can be also confirmed\", \"link\": \"http://arxiv.org/abs/2204.07780v1\", \"pdf_url\": \"https://arxiv.org/pdf/abs/2204.07780v1.pdf\", \"journal_ref\": null}, {\"id\": \"2305.09880v4\", \"title\": \"A survey of the Vision Transformers and their CNN-Transformer based\\\\n  Variants\", \"category\": \"cs.CV\", \"authors\": [\"Asifullah Khan\", \"Zunaira Rauf\", \"Anabia Sohail\", \"Abdul Rehman\", \"Hifsa Asif\", \"Aqsa Asif\", \"Umair Farooq\"], \"author\\'s comment\": \"Pages: 84, Figures: 16\", \"published\": \"2023-05-17T01:27:27Z\", \"summary\": \"Vision transformers have become popular as a possible substitute to\\\\nconvolutional neural networks (CNNs) for a variety of computer vision\\\\napplications. These transformers, with their ability to focus on global\\\\nrelationships in images, offer large learning capacity. However, they may\\\\nsuffer from limited generalization as they do not tend to model local\\\\ncorrelation in images. Recently, in vision transformers hybridization of both\\\\nthe convolution operation and self-attention mechanism has emerged, to exploit\\\\nboth the local and global image representations. These hybrid vision\\\\ntransformers, also referred to as CNN-Transformer architectures, have\\\\ndemonstrated remarkable results in vision applications. Given the rapidly\\\\ngrowing number of hybrid vision transformers, it has become necessary to\\\\nprovide a taxonomy and explanation of these hybrid architectures. This survey\\\\npresents a taxonomy of the recent vision transformer architectures and more\\\\nspecifically that of the hybrid vision transformers. Additionally, the key\\\\nfeatures of these architectures such as the attention mechanisms, positional\\\\nembeddings, multi-scale processing, and convolution are also discussed. In\\\\ncontrast to the previous survey papers that are primarily focused on individual\\\\nvision transformer architectures or CNNs, this survey uniquely emphasizes the\\\\nemerging trend of hybrid vision transformers. By showcasing the potential of\\\\nhybrid vision transformers to deliver exceptional performance across a range of\\\\ncomputer vision tasks, this survey sheds light on the future directions of this\\\\nrapidly evolving architecture.\", \"link\": \"http://arxiv.org/abs/2305.09880v4\", \"pdf_url\": \"https://arxiv.org/pdf/abs/2305.09880v4.pdf\", \"journal_ref\": \"Artificial Intelligence Review (2023): 1-54\"}, {\"id\": \"2205.10660v1\", \"title\": \"Vision Transformers in 2022: An Update on Tiny ImageNet\", \"category\": \"cs.CV\", \"authors\": [\"Ethan Huynh\"], \"author\\'s comment\": \"No comments available\", \"published\": \"2022-05-21T19:48:28Z\", \"summary\": \"The recent advances in image transformers have shown impressive results and\\\\nhave largely closed the gap between traditional CNN architectures. The standard\\\\nprocedure is to train on large datasets like ImageNet-21k and then finetune on\\\\nImageNet-1k. After finetuning, researches will often consider the transfer\\\\nlearning performance on smaller datasets such as CIFAR-10/100 but have left out\\\\nTiny ImageNet. This paper offers an update on vision transformers\\' performance\\\\non Tiny ImageNet. I include Vision Transformer (ViT) , Data Efficient Image\\\\nTransformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin\\\\nTransformers. In addition, Swin Transformers beats the current state-of-the-art\\\\nresult with a validation accuracy of 91.35%. Code is available here:\\\\nhttps://github.com/ehuynh1106/TinyImageNet-Transformers\", \"link\": \"http://arxiv.org/abs/2205.10660v1\", \"pdf_url\": \"https://arxiv.org/pdf/abs/2205.10660v1.pdf\", \"journal_ref\": null}, {\"id\": \"2307.09402v1\", \"title\": \"Study of Vision Transformers for Covid-19 Detection from Chest X-rays\", \"category\": \"eess.IV\", \"authors\": [\"Sandeep Angara\", \"Sharath Thirunagaru\"], \"author\\'s comment\": \"No comments available\", \"published\": \"2023-07-17T14:06:07Z\", \"summary\": \"The COVID-19 pandemic has led to a global health crisis, highlighting the\\\\nneed for rapid and accurate virus detection. This research paper examines\\\\ntransfer learning with vision transformers for COVID-19 detection, known for\\\\nits excellent performance in image recognition tasks. We leverage the\\\\ncapability of Vision Transformers to capture global context and learn complex\\\\npatterns from chest X-ray images. In this work, we explored the recent\\\\nstate-of-art transformer models to detect Covid-19 using CXR images such as\\\\nvision transformer (ViT), Swin-transformer, Max vision transformer (MViT), and\\\\nPyramid Vision transformer (PVT). Through the utilization of transfer learning\\\\nwith IMAGENET weights, the models achieved an impressive accuracy range of\\\\n98.75% to 99.5%. Our experiments demonstrate that Vision Transformers achieve\\\\nstate-of-the-art performance in COVID-19 detection, outperforming traditional\\\\nmethods and even Convolutional Neural Networks (CNNs). The results highlight\\\\nthe potential of Vision Transformers as a powerful tool for COVID-19 detection,\\\\nwith implications for improving the efficiency and accuracy of screening and\\\\ndiagnosis in clinical settings.\", \"link\": \"http://arxiv.org/abs/2307.09402v1\", \"pdf_url\": \"https://arxiv.org/pdf/abs/2307.09402v1.pdf\", \"journal_ref\": null}, {\"id\": \"2402.05557v1\", \"title\": \"On Convolutional Vision Transformers for Yield Prediction\", \"category\": \"cs.CV\", \"authors\": [\"Alvin Inderka\", \"Florian Huber\", \"Volker Steinhage\"], \"author\\'s comment\": \"No comments available\", \"published\": \"2024-02-08T10:50:12Z\", \"summary\": \"While a variety of methods offer good yield prediction on histogrammed remote\\\\nsensing data, vision Transformers are only sparsely represented in the\\\\nliterature. The Convolution vision Transformer (CvT) is being tested to\\\\nevaluate vision Transformers that are currently achieving state-of-the-art\\\\nresults in many other vision tasks. CvT combines some of the advantages of\\\\nconvolution with the advantages of dynamic attention and global context fusion\\\\nof Transformers. It performs worse than widely tested methods such as XGBoost\\\\nand CNNs, but shows that Transformers have potential to improve yield\\\\nprediction.\", \"link\": \"http://arxiv.org/abs/2402.05557v1\", \"pdf_url\": \"https://arxiv.org/pdf/abs/2402.05557v1.pdf\", \"journal_ref\": null}]', name='Arxiv_Retriever', id='fc675e2c-782d-4414-b2f1-e63b88752414', tool_call_id='hVebn5ZGU'),\n AIMessage(content='Vision transformers have gained popularity as a potential alternative to convolutional neural networks (CNNs) for various computer vision tasks. They have the ability to focus on global relationships in images, providing a large learning capacity. However, they might struggle with modeling local correlations in images, leading to limited generalization.\\n\\nTo address this, hybrid vision transformers, also known as CNN-Transformer architectures, have been developed. These models combine the convolution operation and self-attention mechanism to exploit both local and global image representations. A survey titled \"A survey of the Vision Transformers and their CNN-Transformer based Variants\" provides a taxonomy of these hybrid architectures and discusses their key features.\\n\\nAnother research paper titled \"Towards Lightweight Transformer via Group-wise Transformation for Vision-and-Language Tasks\" introduces a lightweight transformer for vision-and-language tasks. This model, termed LW-Transformer, applies group-wise transformation to reduce both the parameters and computations of the transformer while preserving its main properties.\\n\\nIn the context of COVID-19 detection, a study titled \"Study of Vision Transformers for Covid-19 Detection from Chest X-rays\" leverages the capability of vision transformers to capture global context and learn complex patterns from chest X-ray images. The results show that vision transformers achieve state-of-the-art performance in COVID-19 detection, outperforming traditional methods and even CNNs.\\n\\nFor yield prediction, a paper titled \"On Convolutional Vision Transformers for Yield Prediction\" tests the Convolutional Vision Transformer (CvT), which combines some of the advantages of convolution with the advantages of dynamic attention and global context fusion of transformers. While it performs worse than widely tested methods such as XGBoost and CNNs, it shows that transformers have potential to improve yield prediction.\\n\\nThese research papers highlight the versatility and potential of vision transformers in various applications.', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 2444, 'total_tokens': 2881, 'completion_tokens': 437}, 'model': 'mistral-large-2402', 'finish_reason': 'stop'}, id='run-b4a71fbf-9727-4f4e-9215-765b0e570aa3-0', usage_metadata={'input_tokens': 2444, 'output_tokens': 437, 'total_tokens': 2881})]"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"messages\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-10T21:47:21.840195200Z",
     "start_time": "2024-11-10T21:47:21.835821200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "{'messages': [HumanMessage(content='Tell me about vision transformers, using any relevant research paper', additional_kwargs={}, response_metadata={}, id='e4753bc6-b2b1-4e7e-bf0a-b7762e39dd42'),\n  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'hVebn5ZGU', 'type': 'function', 'function': {'name': 'Arxiv_Retriever', 'arguments': '{\"__arg1\": \"vision transformers\"}'}}]}, response_metadata={'token_usage': {'prompt_tokens': 97, 'total_tokens': 126, 'completion_tokens': 29}, 'model': 'mistral-large-2402', 'finish_reason': 'tool_calls'}, id='run-786a47e7-ebf0-47ea-8ee0-ffcadd7f651d-0', tool_calls=[{'name': 'Arxiv_Retriever', 'args': {'__arg1': 'vision transformers'}, 'id': 'hVebn5ZGU', 'type': 'tool_call'}], usage_metadata={'input_tokens': 97, 'output_tokens': 29, 'total_tokens': 126}),\n  ToolMessage(content='[{\"id\": \"2204.07780v1\", \"title\": \"Towards Lightweight Transformer via Group-wise Transformation for\\\\n  Vision-and-Language Tasks\", \"category\": \"cs.CV\", \"authors\": [\"Gen Luo\", \"Yiyi Zhou\", \"Xiaoshuai Sun\", \"Yan Wang\", \"Liujuan Cao\", \"Yongjian Wu\", \"Feiyue Huang\", \"Rongrong Ji\"], \"author\\'s comment\": \"No comments available\", \"published\": \"2022-04-16T11:30:26Z\", \"summary\": \"Despite the exciting performance, Transformer is criticized for its excessive\\\\nparameters and computation cost. However, compressing Transformer remains as an\\\\nopen problem due to its internal complexity of the layer designs, i.e.,\\\\nMulti-Head Attention (MHA) and Feed-Forward Network (FFN). To address this\\\\nissue, we introduce Group-wise Transformation towards a universal yet\\\\nlightweight Transformer for vision-and-language tasks, termed as\\\\nLW-Transformer. LW-Transformer applies Group-wise Transformation to reduce both\\\\nthe parameters and computations of Transformer, while also preserving its two\\\\nmain properties, i.e., the efficient attention modeling on diverse subspaces of\\\\nMHA, and the expanding-scaling feature transformation of FFN. We apply\\\\nLW-Transformer to a set of Transformer-based networks, and quantitatively\\\\nmeasure them on three vision-and-language tasks and six benchmark datasets.\\\\nExperimental results show that while saving a large number of parameters and\\\\ncomputations, LW-Transformer achieves very competitive performance against the\\\\noriginal Transformer networks for vision-and-language tasks. To examine the\\\\ngeneralization ability, we also apply our optimization strategy to a recently\\\\nproposed image Transformer called Swin-Transformer for image classification,\\\\nwhere the effectiveness can be also confirmed\", \"link\": \"http://arxiv.org/abs/2204.07780v1\", \"pdf_url\": \"https://arxiv.org/pdf/abs/2204.07780v1.pdf\", \"journal_ref\": null}, {\"id\": \"2305.09880v4\", \"title\": \"A survey of the Vision Transformers and their CNN-Transformer based\\\\n  Variants\", \"category\": \"cs.CV\", \"authors\": [\"Asifullah Khan\", \"Zunaira Rauf\", \"Anabia Sohail\", \"Abdul Rehman\", \"Hifsa Asif\", \"Aqsa Asif\", \"Umair Farooq\"], \"author\\'s comment\": \"Pages: 84, Figures: 16\", \"published\": \"2023-05-17T01:27:27Z\", \"summary\": \"Vision transformers have become popular as a possible substitute to\\\\nconvolutional neural networks (CNNs) for a variety of computer vision\\\\napplications. These transformers, with their ability to focus on global\\\\nrelationships in images, offer large learning capacity. However, they may\\\\nsuffer from limited generalization as they do not tend to model local\\\\ncorrelation in images. Recently, in vision transformers hybridization of both\\\\nthe convolution operation and self-attention mechanism has emerged, to exploit\\\\nboth the local and global image representations. These hybrid vision\\\\ntransformers, also referred to as CNN-Transformer architectures, have\\\\ndemonstrated remarkable results in vision applications. Given the rapidly\\\\ngrowing number of hybrid vision transformers, it has become necessary to\\\\nprovide a taxonomy and explanation of these hybrid architectures. This survey\\\\npresents a taxonomy of the recent vision transformer architectures and more\\\\nspecifically that of the hybrid vision transformers. Additionally, the key\\\\nfeatures of these architectures such as the attention mechanisms, positional\\\\nembeddings, multi-scale processing, and convolution are also discussed. In\\\\ncontrast to the previous survey papers that are primarily focused on individual\\\\nvision transformer architectures or CNNs, this survey uniquely emphasizes the\\\\nemerging trend of hybrid vision transformers. By showcasing the potential of\\\\nhybrid vision transformers to deliver exceptional performance across a range of\\\\ncomputer vision tasks, this survey sheds light on the future directions of this\\\\nrapidly evolving architecture.\", \"link\": \"http://arxiv.org/abs/2305.09880v4\", \"pdf_url\": \"https://arxiv.org/pdf/abs/2305.09880v4.pdf\", \"journal_ref\": \"Artificial Intelligence Review (2023): 1-54\"}, {\"id\": \"2205.10660v1\", \"title\": \"Vision Transformers in 2022: An Update on Tiny ImageNet\", \"category\": \"cs.CV\", \"authors\": [\"Ethan Huynh\"], \"author\\'s comment\": \"No comments available\", \"published\": \"2022-05-21T19:48:28Z\", \"summary\": \"The recent advances in image transformers have shown impressive results and\\\\nhave largely closed the gap between traditional CNN architectures. The standard\\\\nprocedure is to train on large datasets like ImageNet-21k and then finetune on\\\\nImageNet-1k. After finetuning, researches will often consider the transfer\\\\nlearning performance on smaller datasets such as CIFAR-10/100 but have left out\\\\nTiny ImageNet. This paper offers an update on vision transformers\\' performance\\\\non Tiny ImageNet. I include Vision Transformer (ViT) , Data Efficient Image\\\\nTransformer (DeiT), Class Attention in Image Transformer (CaiT), and Swin\\\\nTransformers. In addition, Swin Transformers beats the current state-of-the-art\\\\nresult with a validation accuracy of 91.35%. Code is available here:\\\\nhttps://github.com/ehuynh1106/TinyImageNet-Transformers\", \"link\": \"http://arxiv.org/abs/2205.10660v1\", \"pdf_url\": \"https://arxiv.org/pdf/abs/2205.10660v1.pdf\", \"journal_ref\": null}, {\"id\": \"2307.09402v1\", \"title\": \"Study of Vision Transformers for Covid-19 Detection from Chest X-rays\", \"category\": \"eess.IV\", \"authors\": [\"Sandeep Angara\", \"Sharath Thirunagaru\"], \"author\\'s comment\": \"No comments available\", \"published\": \"2023-07-17T14:06:07Z\", \"summary\": \"The COVID-19 pandemic has led to a global health crisis, highlighting the\\\\nneed for rapid and accurate virus detection. This research paper examines\\\\ntransfer learning with vision transformers for COVID-19 detection, known for\\\\nits excellent performance in image recognition tasks. We leverage the\\\\ncapability of Vision Transformers to capture global context and learn complex\\\\npatterns from chest X-ray images. In this work, we explored the recent\\\\nstate-of-art transformer models to detect Covid-19 using CXR images such as\\\\nvision transformer (ViT), Swin-transformer, Max vision transformer (MViT), and\\\\nPyramid Vision transformer (PVT). Through the utilization of transfer learning\\\\nwith IMAGENET weights, the models achieved an impressive accuracy range of\\\\n98.75% to 99.5%. Our experiments demonstrate that Vision Transformers achieve\\\\nstate-of-the-art performance in COVID-19 detection, outperforming traditional\\\\nmethods and even Convolutional Neural Networks (CNNs). The results highlight\\\\nthe potential of Vision Transformers as a powerful tool for COVID-19 detection,\\\\nwith implications for improving the efficiency and accuracy of screening and\\\\ndiagnosis in clinical settings.\", \"link\": \"http://arxiv.org/abs/2307.09402v1\", \"pdf_url\": \"https://arxiv.org/pdf/abs/2307.09402v1.pdf\", \"journal_ref\": null}, {\"id\": \"2402.05557v1\", \"title\": \"On Convolutional Vision Transformers for Yield Prediction\", \"category\": \"cs.CV\", \"authors\": [\"Alvin Inderka\", \"Florian Huber\", \"Volker Steinhage\"], \"author\\'s comment\": \"No comments available\", \"published\": \"2024-02-08T10:50:12Z\", \"summary\": \"While a variety of methods offer good yield prediction on histogrammed remote\\\\nsensing data, vision Transformers are only sparsely represented in the\\\\nliterature. The Convolution vision Transformer (CvT) is being tested to\\\\nevaluate vision Transformers that are currently achieving state-of-the-art\\\\nresults in many other vision tasks. CvT combines some of the advantages of\\\\nconvolution with the advantages of dynamic attention and global context fusion\\\\nof Transformers. It performs worse than widely tested methods such as XGBoost\\\\nand CNNs, but shows that Transformers have potential to improve yield\\\\nprediction.\", \"link\": \"http://arxiv.org/abs/2402.05557v1\", \"pdf_url\": \"https://arxiv.org/pdf/abs/2402.05557v1.pdf\", \"journal_ref\": null}]', name='Arxiv_Retriever', id='fc675e2c-782d-4414-b2f1-e63b88752414', tool_call_id='hVebn5ZGU'),\n  AIMessage(content='Vision transformers have gained popularity as a potential alternative to convolutional neural networks (CNNs) for various computer vision tasks. They have the ability to focus on global relationships in images, providing a large learning capacity. However, they might struggle with modeling local correlations in images, leading to limited generalization.\\n\\nTo address this, hybrid vision transformers, also known as CNN-Transformer architectures, have been developed. These models combine the convolution operation and self-attention mechanism to exploit both local and global image representations. A survey titled \"A survey of the Vision Transformers and their CNN-Transformer based Variants\" provides a taxonomy of these hybrid architectures and discusses their key features.\\n\\nAnother research paper titled \"Towards Lightweight Transformer via Group-wise Transformation for Vision-and-Language Tasks\" introduces a lightweight transformer for vision-and-language tasks. This model, termed LW-Transformer, applies group-wise transformation to reduce both the parameters and computations of the transformer while preserving its main properties.\\n\\nIn the context of COVID-19 detection, a study titled \"Study of Vision Transformers for Covid-19 Detection from Chest X-rays\" leverages the capability of vision transformers to capture global context and learn complex patterns from chest X-ray images. The results show that vision transformers achieve state-of-the-art performance in COVID-19 detection, outperforming traditional methods and even CNNs.\\n\\nFor yield prediction, a paper titled \"On Convolutional Vision Transformers for Yield Prediction\" tests the Convolutional Vision Transformer (CvT), which combines some of the advantages of convolution with the advantages of dynamic attention and global context fusion of transformers. While it performs worse than widely tested methods such as XGBoost and CNNs, it shows that transformers have potential to improve yield prediction.\\n\\nThese research papers highlight the versatility and potential of vision transformers in various applications.', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 2444, 'total_tokens': 2881, 'completion_tokens': 437}, 'model': 'mistral-large-2402', 'finish_reason': 'stop'}, id='run-b4a71fbf-9727-4f4e-9215-765b0e570aa3-0', usage_metadata={'input_tokens': 2444, 'output_tokens': 437, 'total_tokens': 2881}),\n  HumanMessage(content='How does vision transformers are different than text Transformers?', additional_kwargs={}, response_metadata={}, id='38c004f7-06bd-41a0-aafb-9ee82403f2e6'),\n  AIMessage(content='Vision transformers (ViT) and text transformers have fundamental differences in their architecture and input processing, as they are designed to handle different types of data.\\n\\n1. Input representation: Text transformers work with 1D sequences of tokens, representing words or subwords in a sentence. In contrast, vision transformers process 2D image data. To adapt images for processing in a transformer, images are divided into fixed-size patches, which are then flattened and linearly embedded to form a sequence of tokens.\\n2. Positional encoding: Both text and vision transformers use positional encoding to provide information about the order of the input tokens. In text transformers, positional encoding is added to the token embeddings to preserve the word order. In vision transformers, positional encoding is used to maintain the spatial information of the image patches.\\n3. Architecture: While text transformers typically use an encoder-decoder architecture for tasks like machine translation, vision transformers usually rely only on the encoder part. The encoder consists of multiple self-attention layers that enable the model to capture global dependencies in the input data.\\n\\nIn summary, vision transformers differ from text transformers in their input representation, positional encoding, and architecture, as they are tailored to handle 2D image data rather than 1D text sequences.\\n\\nI can search for research papers related to this topic if you would like more in-depth information.', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 550, 'total_tokens': 856, 'completion_tokens': 306}, 'model': 'mistral-large-2402', 'finish_reason': 'stop'}, id='run-704205ce-3b14-4017-b425-d1be716f229b-0', usage_metadata={'input_tokens': 550, 'output_tokens': 306, 'total_tokens': 856})]}"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"messages\": HumanMessage(\"How does vision transformers are different than text Transformers?\")}, config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-10T21:47:29.679342900Z",
     "start_time": "2024-11-10T21:47:21.841208200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
